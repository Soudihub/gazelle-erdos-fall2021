{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8dc705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c8891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import requests\n",
    "import sys\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import ast\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.text import Text\n",
    "from datetime import datetime, timedelta\n",
    "import string\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26665078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d338e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, prediction):\n",
    "    return np.sqrt(np.sum(np.power(true-prediction,2))/len(true))\n",
    "\n",
    "def mean_err(true, prediction):\n",
    "    return np.sum(true-prediction)/len(true)\n",
    "\n",
    "def powerset_no_empty(s):\n",
    "    power_set = []\n",
    "    x = len(s)\n",
    "    for i in range(1 << x):\n",
    "        power_set.append([s[j] for j in range(x) if (i & (1 << j))])\n",
    "            \n",
    "    return power_set[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe8deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection to the WSB database file\n",
    "conn = sqlite3.connect(\"reddit_wallstreetbets.db\")\n",
    "\n",
    "# create our cursor (this allows us to execute SQL code chunks written as python strings)\n",
    "c = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10efb9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_posts now has 6144 entries.\n",
      "post_stats now has 129693 entries.\n"
     ]
    }
   ],
   "source": [
    "#Print out database numbers before ending iteration\n",
    "c.execute(\"SELECT * FROM new_posts\")\n",
    "new_posts_df = pd.DataFrame(c.fetchall(), columns = [x[0] for x in c.description])\n",
    "print( 'new_posts now has '+str(len(new_posts_df))+' entries.' )\n",
    "c.execute(\"SELECT * FROM post_stats\")\n",
    "post_stats_df = pd.DataFrame(c.fetchall(), columns = [x[0] for x in c.description])\n",
    "print( 'post_stats now has '+str(len(post_stats_df))+' entries.' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fdc7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_entries = []\n",
    "upvotes_24hrs = []\n",
    "top_hot_loc = []\n",
    "for i in range(len(new_posts_df)):\n",
    "    c.execute(\"SELECT upvotes FROM post_stats where hour=24 and post_id=\"+str(i))\n",
    "    fetch_val = c.fetchall()\n",
    "    if len( fetch_val ) < 1:\n",
    "        incomplete_entries.append(i)\n",
    "    else:\n",
    "        upvotes_24hrs.append( fetch_val[0][0] )\n",
    "    c.execute(\"SELECT hot_val FROM post_stats where post_id=\"+str(i))\n",
    "    top_hot_loc.append( min(c.fetchall())[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc268c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vals = []\n",
    "days = []\n",
    "for i in range(len(new_posts_df)):\n",
    "    time_str = new_posts_df[\"submit_time\"][i].split('T')[1]\n",
    "    time_val = float(time_str.split(':')[0])+float(time_str.split(':')[1])/60.+float(time_str.split(':')[2])/360.\n",
    "    time_vals.append(time_val)\n",
    "    days.append( pd.Timestamp( new_posts_df[\"submit_time\"][i].split('T')[0].replace('\"','') ).day_name() )\n",
    "\n",
    "new_posts_df['submit_hour'] = time_vals\n",
    "new_posts_df['submit_day'] = days\n",
    "new_posts_df['best_hot_val'] = top_hot_loc\n",
    "\n",
    "WSB_df = new_posts_df.drop(incomplete_entries)\n",
    "WSB_df['upvotes_tot'] = upvotes_24hrs\n",
    "\n",
    "WSB_df['redditor_for'] = WSB_df['redditor_for'] - min(WSB_df['redditor_for'])\n",
    "\n",
    "\n",
    "#Need a definition of 'viral'\n",
    "#Let's go with more than 2,500 upvotes or making it to the top 5 of the page\n",
    "viral = []\n",
    "for post_id in WSB_df['post_id']:\n",
    "    if (WSB_df['upvotes_tot'][post_id] >= 2500):\n",
    "        viral.append(1)\n",
    "    elif (WSB_df['best_hot_val'][post_id] <= 5):\n",
    "        viral.append(1)\n",
    "    else:\n",
    "        viral.append(0)\n",
    "\n",
    "WSB_df['viral'] = viral\n",
    "\n",
    "#Let's ignore daily discussion threads, since they are at the top of the page but not really viral\n",
    "WSB_df = WSB_df.drop(WSB_df[WSB_df['flair']=='Daily Discussion'].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5e47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get one-hot encoded day and flair variables and add them to the df\n",
    "WSB_df = pd.concat([WSB_df, pd.get_dummies(WSB_df['flair'])], axis=1)\n",
    "WSB_df = pd.concat([WSB_df, pd.get_dummies(WSB_df['submit_day'])], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56b2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = WSB_df['title']\n",
    "\n",
    "symbolset = string.punctuation\n",
    "\n",
    "n = len(title)\n",
    "WSB_df[\"propernouns\"] = np.zeros(n)\n",
    "WSB_df[\"numbers\"] = np.zeros(n)\n",
    "WSB_df[\"hashtags\"] = np.zeros(n)\n",
    "WSB_df[\"symbols\"] = np.zeros(n)\n",
    "for i in title.index:\n",
    "    text = title[i]\n",
    "    words = nltk.word_tokenize(text)\n",
    "    taggedtoken = nltk.pos_tag(words)\n",
    "    NPcount = 0\n",
    "    NUMcount = 0\n",
    "    SYMcount = 0\n",
    "    HASHcount = 0\n",
    "    for word in taggedtoken:\n",
    "        if word[1] == \"NNP\" or word[1] == \"NNPS\":\n",
    "            NPcount += 1\n",
    "        if word[1] == \"JJ\" or word[1] == \"CD\":\n",
    "            NUMcount += 1\n",
    "        if word[0] == \"#\":\n",
    "            HASHcount += 1\n",
    "        if word[0] in symbolset :\n",
    "            SYMcount += 1\n",
    "    WSB_df[\"propernouns\"][i] = NPcount\n",
    "    WSB_df[\"numbers\"][i] = NUMcount\n",
    "    WSB_df[\"hashtags\"][i] = HASHcount\n",
    "    WSB_df[\"symbols\"][i] = SYMcount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50eefc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a train/test split\n",
    "WSB_df_train, WSB_df_test = train_test_split(WSB_df, shuffle=True, random_state=48, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59a5c7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'active_track', 'title', 'comment_url', 'link_url', 'flair',\n",
       "       'submit_time', 'rising_val', 'hot_val', 'username', 'post_karma',\n",
       "       'comment_karma', 'redditor_for', 'upvotes', 'upvote_percent',\n",
       "       'num_comments', 'submit_hour', 'submit_day', 'best_hot_val',\n",
       "       'upvotes_tot', 'viral', 'DD', 'Discussion', 'Earnings Thread', 'Gain',\n",
       "       'Loss', 'Meme', 'Mods', 'News', 'None', 'Shitpost',\n",
       "       'Technical Analysis', 'Weekend Discussion', 'YOLO', 'Friday', 'Monday',\n",
       "       'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'propernouns',\n",
       "       'numbers', 'hashtags', 'symbols'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WSB_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c9a4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['rising_val', 'hot_val', 'post_karma',\n",
    "       'comment_karma', 'redditor_for', 'upvotes', 'upvote_percent',\n",
    "       'num_comments', 'submit_hour','DD', 'Discussion', 'Earnings Thread', 'Gain',\n",
    "       'Loss', 'Meme', 'Mods', 'News', 'None', 'Shitpost',\n",
    "       'Technical Analysis', 'Weekend Discussion', 'YOLO', 'Friday', 'Monday',\n",
    "       'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'propernouns',\n",
    "       'numbers', 'hashtags', 'symbols']\n",
    "\n",
    "#Let's do some k-fold cross-validation\n",
    "n_k = 5\n",
    "#kf = KFold(n_k)\n",
    "kf = StratifiedKFold(n_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e4863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% done with k-fold iterations.\n"
     ]
    }
   ],
   "source": [
    "maxdepths = range(5,35)\n",
    "FN_RFC = np.zeros((n_k,len(maxdepths)))\n",
    "FP_RFC = np.zeros((n_k,len(maxdepths)))\n",
    "i = 0\n",
    "count_int = 0.0\n",
    "for train_index, test_index in kf.split(WSB_df_train,WSB_df_train['upvotes_tot']):\n",
    "    global count_int\n",
    "    #train_index = list(kf.split(WSB_df_train,WSB_df_train['upvotes_tot']))[i][0]\n",
    "    #test_index = list(kf.split(WSB_df_train,WSB_df_train['upvotes_tot']))[i][1]\n",
    "    df_train_train = WSB_df_train.iloc[train_index]\n",
    "    df_holdout = WSB_df_train.iloc[test_index]\n",
    "    j = 0\n",
    "    for maxdepth in maxdepths:\n",
    "        RFC = RandomForestClassifier(max_depth = maxdepth)\n",
    "        RFC.fit(WSB_df_train[predictors], WSB_df_train['viral'])\n",
    "        pred = RFC.predict(df_holdout[predictors])\n",
    "        \n",
    "        RFC_matrix = confusion_matrix(df_holdout['viral'], pred)\n",
    "        FP = RFC_matrix[0,1]\n",
    "        FN = RFC_matrix[1,0]\n",
    "        \n",
    "        FN_RFC[i,j] = FN\n",
    "        FP_RFC[i,j] = FP\n",
    "        \n",
    "        count_int += 1.0\n",
    "        percent = round(100.0*count_int/(n_k*len(maxdepths)),2)\n",
    "        clear_output()\n",
    "        print(str(percent)+'% done with k-fold iterations.')\n",
    "        \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a87bca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([44.4, 39.8, 33.6, 29.8, 23.8, 20. , 16. , 11.2,  7.6,  4. ,  0.6,\n",
       "        0.6,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0.2,  0. ,  0. ,  0. ,  0. ,  0. ,  0.2,  0. ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_depth = maxdepths[np.argmin(np.mean(FN_RFC+FP_RFC, axis = 0))]\n",
    "print(best_depth)\n",
    "\n",
    "np.mean(FN_RFC+FP_RFC, axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad217237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3669    0]\n",
      " [   0  297]]\n",
      "[[924  11]\n",
      " [ 33  24]]\n"
     ]
    }
   ],
   "source": [
    "#Check the best_depth RFC\n",
    "RFC = RandomForestClassifier(max_depth = best_depth)\n",
    "\n",
    "# fit the model\n",
    "RFC.fit(WSB_df_train[predictors], WSB_df_train['viral'])\n",
    "\n",
    "pred_train = RFC.predict(WSB_df_train[predictors])\n",
    "\n",
    "RFC_matrix = confusion_matrix(WSB_df_train['viral'], pred_train)\n",
    "TN = RFC_matrix[0,0]\n",
    "FP = RFC_matrix[0,1]\n",
    "FN = RFC_matrix[1,0]\n",
    "TP = RFC_matrix[1,1]\n",
    "\n",
    "print(RFC_matrix)\n",
    "\n",
    "pred_test = RFC.predict(WSB_df_test[predictors])\n",
    "print(confusion_matrix(WSB_df_test['viral'], pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c39e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b9666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
